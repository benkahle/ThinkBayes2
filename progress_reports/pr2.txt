Ben Kahle
9/18/14
Bayesian
Progress Report 2

	Chapter 4 of Think Bayes provided a few examples of Bayesian analysis and computation that I found particularly interesting. To begin, the euro problem is a great instance of a problem that is more complicated than it seems at the surface. While it is fairly trivial to determine the likelihood of a heads given the data observed, finding the answer to the more important question, if the data indicates a biased coin, is less obvious. The euro problem is also a clear example of priors being overridden by data. Believing that all probabilities for heads vs. tail would be an even distribution seems very unlikely and as a result, initially seems like it would create an error in any calculation that included it. Consequently, I was surprised to see the degree to which a uniform distribution and a triangular distribution lead to the same result.
	A section of chapter 4 that was very useful to me was understanding different ways to optimize the computation of the likelihood. I really like the approach of getting a clean and more clearly correct version of the function to work and then returning with the goal of optimization. Pre-mature optimization can often make many problems difficult to understand, harder to debug, and ultimately a longer process to develop. I really enjoyed seeing the progression from the original function to the more concise function over each iteration. The process gave a great view into your thought process when it comes to algorithmic optimization and it seems like a great approach for a much broader set of circumstances than just this problem. Of course at a certain point, as demonstrated by this problem, to make a more significant optimization, the overall approach must be reconsidered as well. In this case, considering the beta distribution was an intriguing method of reducing the computation of the problem to a couple addition problems.
	I found the Lincoln Index blog post to be a really cool example of a problem I did not anticipate to be approachable from a statistics point of view. However, even with just the week or so of class it was very clear how the problem could be broken down with a Bayesian approach. While I haven’t had a chance this week, I am interested in looking into a method to extend the solution to work for additional numbers of testers. It would be very interesting to see how the number of testers effects the width of the credibility range. Additionally I’m curious about the possibility of adding a parameter of difficulty to the bugs. This would make the likelihood function much more difficult to determine, but would make the solution much more realistic.
	Within chapter 5, I found the Oliver’s Blood problem to be very counterintuitive. At first reading it seems very unlikely that data consistent with a hypothesis could actually be evidence against the hypothesis. Even with the additional perspective of finding an ‘AB’ blood type from two random people vs one random person, I still initially found the idea unconvincing. It took me a few re-readings to start believing the example. I still find these counterintuitive aspects of statistics incredibly interesting and really fun to work through.
	One of my goals this week was to explore NumPy and SciPy more thoroughly than I have in the passed. I have used both libraries for different projects in the past but I have never put much effort into learning the extent to which they can provide useful functions and data structures for problems I may be solving. I read through a portion of the NumPy documentation as well as a high level tutorial on the library on wiki.scipy.org which demonstrated some of the common uses of the array and matrix data types and corresponding methods. Having spent a bit of time recently relearning MatLab syntax for Linearity 2, I found it very interesting to explore the NumPy linalg module and see some of the slight differences from MatLab. Additionally, I explored some of the linear optimization tools offered in SciPy in comparison to the linear optimization toolbox of MatLab that I have been using recently. Now that I have a bit of familiarity with the offerings of both libraries as well as MatLab’s offerings I am interested in spending some time to compare the two approaches more closely. Specifically I am curious to see which language offers a more approachable set of tools for linear, and non-linear optimization and if there is any significant speed differences between the two. My initial assumption would be the MatLab, as a fairly specialized program would have a significant boost in computational speed above python, but I would be interested to see how well python can compete (or maybe outperform!).
	Another goal of mine for this week was to explore some new areas of statistical inquiries to see if I could find more intriguing topics to explore deeper. One study I found compared the salaries of men and women in the field of engineering. The study found that while many nationwide studies suggest that women earn between 71-74 cents for every dollar that men earn, within the field of engineering, and while controlling for level of education, women earn a median of 97 cents for each dollar that men earn. This disparity is interesting for a couple of reasons. For one, the nationwide study numbers are often cited in many widespread articles and I am curious to see how the numbers would change when controlling for different fields and levels of education. Secondly, as a field that is notorious for a lack of women it seems unintuitive that engineering would have better gender salary equity than other fields. I would be interested to see if there is statistical significance to these numbers and if there may be other underlying information that could be uncovered by analyzing employment, salary, and education data from many industries. ￼ http://www.nsf.gov/statistics/issuebrf/sib99352.htm